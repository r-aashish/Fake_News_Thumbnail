{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEpMujdzwWI+gCY9JP2FFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-aashish/Fake_News_Thumbnail/blob/main/Project_Final_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AowQZTQR3RBJ",
        "outputId": "47190100-0eec-4ea1-fa54-27d91ffc6eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.11.2)\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.4.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (8.4.0)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.2)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.27.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2022.12.7)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.12.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=9dc4f8871bac8749c3369addaa5c1ee0267563b59fe50c2ee0ac6182578c744a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3356 sha256=9d68effc3ff8889c912d1ddeb13d2e02121ceb22a220924336a4d57bc9c7d2b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=b91221e5bfe14356767bdf65e214c13e648467d12352f2e7d3c4b11339e4d9e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=aeeb1977ba1825ddcded1cb5ea6327164ea910e51dbdb15ed246c14b7e05dc3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.1\n"
          ]
        }
      ],
      "source": [
        "pip install newspaper3k"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Green Data**"
      ],
      "metadata": {
        "id": "zC6MvmgL4ido"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "To Get the news urls into a json file \n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "from newspaper import Config, Article\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "news_urls = [\n",
        "    \"https://www.foxnews.com/us\",\n",
        "    \"https://www.foxnews.com/category/us/crime\",\n",
        "    \"https://www.foxnews.com/category/us/military\",\n",
        "    \"https://www.foxnews.com/category/us/education\",\n",
        "    \"https://www.foxnews.com/category/us/terror\",\n",
        "    \"https://www.foxnews.com/category/us/immigration\",\n",
        "    \"https://www.foxnews.com/category/us/economy\",\n",
        "    \"https://www.foxnews.com/world\",\n",
        "    \"https://www.foxnews.com/category/world/united-nations\",\n",
        "    \"https://www.foxnews.com/category/world/conflicts\",\n",
        "    \"https://www.foxnews.com/category/world/terrorism\",\n",
        "    \"https://www.foxnews.com/tech\",\n",
        "    \"https://www.foxnews.com/category/tech/topics/security\",\n",
        "    \"https://www.foxnews.com/category/science/archaeology\",\n",
        "    \"https://www.foxnews.com/science\",\n",
        "    \"https://www.foxnews.com/health\",\n",
        "    \"https://www.foxnews.com/category/health/infectious-disease/coronavirus\",\n",
        "    \"https://www.foxnews.com/category/health/healthy-living\",\n",
        "    \"https://www.foxnews.com/category/health/medical-research\",\n",
        "    \"https://www.foxnews.com/category/health/mental-health\",\n",
        "    \"https://www.foxbusiness.com/economy\",\n",
        "    \"https://www.foxbusiness.com/markets\",\n",
        "    \"https://www.foxnews.com/category/entertainment/celebrity-news\",\n",
        "    \"https://www.foxnews.com/category/entertainment/music\",\n",
        "    \"https://www.foxnews.com/category/tech/topics/innovation\",\n",
        "    \"https://www.foxnews.com/category/world/disasters\",\n",
        "    \"https://www.theguardian.com/us\",\n",
        "    \"https://www.theguardian.com/us/commentisfree\",\n",
        "    \"https://www.theguardian.com/us/sport\",\n",
        "    \"https://www.theguardian.com/us/culture\",\n",
        "    \"https://www.theguardian.com/us/lifeandstyle\",\n",
        "    \"https://www.theguardian.com/us/technology\",\n",
        "    \"https://www.theguardian.com/us-news/us-politics\",\n",
        "    \"https://www.theguardian.com/world\",\n",
        "    \"https://www.theguardian.com/fashion\",\n",
        "    \"https://www.theguardian.com/food\",\n",
        "    \"https://www.theguardian.com/lifeandstyle/health-and-wellbeing\",\n",
        "    \"https://www.theguardian.com/science\",\n",
        "    \"https://nypost.com/business/\",\n",
        "    \"https://nypost.com/opinion/\",\n",
        "    \"https://nypost.com/lifestyle/\",\n",
        "    \"https://nypost.com/health/\",\n",
        "    \"https://nypost.com/human-interest/\",\n",
        "    \"https://nypost.com/fashion-and-beauty/\",\n",
        "    \"https://nypost.com/food-and-drink/\",\n",
        "    \"https://www.reuters.com/world/\",\n",
        "    \"https://www.reuters.com/world/africa/\",\n",
        "    \"https://www.reuters.com/world/americas/\",\n",
        "    \"https://www.reuters.com/world/india/\",\n",
        "    \"https://www.reuters.com/world/middle-east/\",\n",
        "    \"https://www.reuters.com/world/uk/\",\n",
        "    \"https://www.reuters.com/world/us/\",\n",
        "    \"https://www.reuters.com/business/environment/\",\n",
        "    \"https://www.reuters.com/business/finance/\",\n",
        "    \"https://www.reuters.com/business/future-of-health/\",\n",
        "    \"https://www.reuters.com/business/retail-consumer/\",\n",
        "    \"https://www.reuters.com/business/media-telecom/\",\n",
        "    \"https://www.reuters.com/business/aerospace-defense/\",\n",
        "    \"https://www.reuters.com/markets/carbon/\",\n",
        "    \"https://www.reuters.com/markets/asia/\",\n",
        "    \"https://www.reuters.com/markets/us/\",\n",
        "    \"https://www.reuters.com/markets/global-market-data/\",\n",
        "    \"https://www.reuters.com/markets/europe/\",\n",
        "    \"https://www.reuters.com/markets/currencies/\",\n",
        "    \"https://www.reuters.com/legal/\",\n",
        "    \"https://www.reuters.com/breakingviews/\",\n",
        "    \"https://www.reuters.com/sports/\",\n",
        "    \"https://slate.com/news-and-politics\",\n",
        "    \"https://slate.com/culture\",\n",
        "    \"https://slate.com/culture/movies\",\n",
        "    \"https://slate.com/culture/music\",\n",
        "    \"https://slate.com/culture/television\",\n",
        "    \"https://slate.com/culture/books\",\n",
        "    \"https://slate.com/culture/sports\",\n",
        "    \"https://slate.com/technology\",\n",
        "    \"https://slate.com/business/moneybox\",\n",
        "    \"https://slate.com/business/metropolis\",\n",
        "    \"https://slate.com/technology/the-industry\",\n",
        "    \"https://slate.com/business/the-media\",\n",
        "    \"https://slate.com/technology/users\",\n",
        "    \"https://slate.com/technology/future-tense\",\n",
        "    \"https://slate.com/technology/science\",\n",
        "    \"https://slate.com/technology/medical-examiner\",\n",
        "    \"https://slate.com/human-interest\",\n",
        "    \"https://slate.com/human-interest/family\",\n",
        "    \"https://slate.com/human-interest/relationships\",\n",
        "    \"https://slate.com/human-interest/work\",\n",
        "    \"https://slate.com/human-interest/downtime\",\n",
        "    \"https://slate.com/human-interest/dear-prudence\",\n",
        "    \"https://slate.com/human-interest/care-and-feeding\",\n",
        "    \"https://slate.com/human-interest/how-to-do-it\",\n",
        "    \"https://www.wnd.com/\",\n",
        "    \"https://www.wnd.com/category/front-page/politics/\",\n",
        "    \"https://www.wnd.com/category/front-page/us/\",\n",
        "    \"https://www.wnd.com/category/front-page/world/\",\n",
        "    \"https://www.wnd.com/category/front-page/faith/\",\n",
        "    \"https://www.wnd.com/category/front-page/health/\",\n",
        "    \"https://www.wnd.com/category/diversions/\",\n",
        "    \"https://www.wnd.com/category/front-page/education/\",\n",
        "    \"https://www.wnd.com/category/opinion/commentary/\",\n",
        "    \"https://www.wnd.com/category/money/\"]\n",
        "\n",
        "def extract_headlines(url):\n",
        "    config = Config()\n",
        "    config.request_timeout = 10\n",
        "    config.browser_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "\n",
        "    article = Article(url, config=config)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "\n",
        "    soup = BeautifulSoup(article.html, 'html.parser')\n",
        "\n",
        "    headlines = []\n",
        "    for headline in soup.find_all('a'):\n",
        "        if headline.text.strip() != '':\n",
        "            headline_url = headline['href'] if 'href' in headline.attrs else ''\n",
        "            absolute_url = urljoin(url, headline_url)\n",
        "\n",
        "            # Check if the headline has at least 10 words\n",
        "            words = headline.text.strip().split()\n",
        "            if len(words) >= 10:\n",
        "                headlines.append({\n",
        "                    \"title\": headline.text.strip(),\n",
        "                    \"url\": absolute_url\n",
        "                })\n",
        "\n",
        "    return headlines\n",
        "\n",
        "def get_random_headlines(news_urls, count):\n",
        "    random_headlines = []\n",
        "    article_counts = {}  # Store the counts of articles scraped from each website\n",
        "\n",
        "    for url in news_urls:\n",
        "        headlines = extract_headlines(url)\n",
        "        random_headlines.extend(random.sample(headlines, min(count, len(headlines))))\n",
        "\n",
        "        # Update the article count for the current website\n",
        "        article_counts[url] = len(headlines)\n",
        "\n",
        "        # Check if the desired count has been reached\n",
        "        if len(random_headlines) >= count:\n",
        "            break\n",
        "\n",
        "    return random_headlines[:count], article_counts\n",
        "\n",
        "# Get 1000 random headlines and article counts\n",
        "random_headlines, article_counts = get_random_headlines(news_urls, 100000)\n",
        "\n",
        "# Save headlines to JSON file\n",
        "output_file = \"Green_data.json\"\n",
        "with open(output_file, 'w') as file:\n",
        "    json.dump(random_headlines, file, indent=4)\n",
        "\n",
        "print(\"Random headlines saved to\", output_file)\n",
        "\n",
        "# Print article counts\n",
        "counts = 0\n",
        "print(\"Article Counts:\")\n",
        "for url, count in article_counts.items():\n",
        "    print(url, \"-\", count)\n",
        "    counts+=count\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6FK9-zs4sVh",
        "outputId": "49c6101c-3ebe-4376-e5d5-ed76d7ee9941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random headlines saved to Green_data.json\n",
            "Article Counts:\n",
            "https://www.foxnews.com/us - 75\n",
            "https://www.foxnews.com/category/us/crime - 20\n",
            "https://www.foxnews.com/category/us/military - 17\n",
            "https://www.foxnews.com/category/us/education - 19\n",
            "https://www.foxnews.com/category/us/terror - 20\n",
            "https://www.foxnews.com/category/us/immigration - 18\n",
            "https://www.foxnews.com/category/us/economy - 16\n",
            "https://www.foxnews.com/world - 47\n",
            "https://www.foxnews.com/category/world/united-nations - 19\n",
            "https://www.foxnews.com/category/world/conflicts - 17\n",
            "https://www.foxnews.com/category/world/terrorism - 18\n",
            "https://www.foxnews.com/tech - 43\n",
            "https://www.foxnews.com/category/tech/topics/security - 16\n",
            "https://www.foxnews.com/category/science/archaeology - 18\n",
            "https://www.foxnews.com/science - 42\n",
            "https://www.foxnews.com/health - 35\n",
            "https://www.foxnews.com/category/health/infectious-disease/coronavirus - 19\n",
            "https://www.foxnews.com/category/health/healthy-living - 21\n",
            "https://www.foxnews.com/category/health/medical-research - 20\n",
            "https://www.foxnews.com/category/health/mental-health - 19\n",
            "https://www.foxbusiness.com/economy - 11\n",
            "https://www.foxbusiness.com/markets - 12\n",
            "https://www.foxnews.com/category/entertainment/celebrity-news - 22\n",
            "https://www.foxnews.com/category/entertainment/music - 20\n",
            "https://www.foxnews.com/category/tech/topics/innovation - 21\n",
            "https://www.foxnews.com/category/world/disasters - 15\n",
            "https://www.theguardian.com/us - 147\n",
            "https://www.theguardian.com/us/commentisfree - 42\n",
            "https://www.theguardian.com/us/sport - 84\n",
            "https://www.theguardian.com/us/culture - 64\n",
            "https://www.theguardian.com/us/lifeandstyle - 16\n",
            "https://www.theguardian.com/us/technology - 119\n",
            "https://www.theguardian.com/us-news/us-politics - 57\n",
            "https://www.theguardian.com/world - 112\n",
            "https://www.theguardian.com/fashion - 72\n",
            "https://www.theguardian.com/food - 59\n",
            "https://www.theguardian.com/lifeandstyle/health-and-wellbeing - 16\n",
            "https://www.theguardian.com/science - 71\n",
            "https://nypost.com/business/ - 26\n",
            "https://nypost.com/opinion/ - 29\n",
            "https://nypost.com/lifestyle/ - 62\n",
            "https://nypost.com/health/ - 25\n",
            "https://nypost.com/human-interest/ - 31\n",
            "https://nypost.com/fashion-and-beauty/ - 28\n",
            "https://nypost.com/food-and-drink/ - 29\n",
            "https://www.reuters.com/world/ - 15\n",
            "https://www.reuters.com/world/africa/ - 13\n",
            "https://www.reuters.com/world/americas/ - 14\n",
            "https://www.reuters.com/world/india/ - 19\n",
            "https://www.reuters.com/world/middle-east/ - 15\n",
            "https://www.reuters.com/world/uk/ - 18\n",
            "https://www.reuters.com/world/us/ - 17\n",
            "https://www.reuters.com/business/environment/ - 14\n",
            "https://www.reuters.com/business/finance/ - 17\n",
            "https://www.reuters.com/business/future-of-health/ - 14\n",
            "https://www.reuters.com/business/retail-consumer/ - 14\n",
            "https://www.reuters.com/business/media-telecom/ - 13\n",
            "https://www.reuters.com/business/aerospace-defense/ - 13\n",
            "https://www.reuters.com/markets/carbon/ - 13\n",
            "https://www.reuters.com/markets/asia/ - 13\n",
            "https://www.reuters.com/markets/us/ - 13\n",
            "https://www.reuters.com/markets/global-market-data/ - 2\n",
            "https://www.reuters.com/markets/europe/ - 12\n",
            "https://www.reuters.com/markets/currencies/ - 7\n",
            "https://www.reuters.com/legal/ - 34\n",
            "https://www.reuters.com/breakingviews/ - 4\n",
            "https://www.reuters.com/sports/ - 13\n",
            "https://slate.com/news-and-politics - 28\n",
            "https://slate.com/culture - 26\n",
            "https://slate.com/culture/movies - 20\n",
            "https://slate.com/culture/music - 20\n",
            "https://slate.com/culture/television - 20\n",
            "https://slate.com/culture/books - 20\n",
            "https://slate.com/culture/sports - 20\n",
            "https://slate.com/technology - 27\n",
            "https://slate.com/business/moneybox - 20\n",
            "https://slate.com/business/metropolis - 20\n",
            "https://slate.com/technology/the-industry - 20\n",
            "https://slate.com/business/the-media - 20\n",
            "https://slate.com/technology/users - 20\n",
            "https://slate.com/technology/future-tense - 24\n",
            "https://slate.com/technology/science - 20\n",
            "https://slate.com/technology/medical-examiner - 20\n",
            "https://slate.com/human-interest - 27\n",
            "https://slate.com/human-interest/family - 20\n",
            "https://slate.com/human-interest/relationships - 20\n",
            "https://slate.com/human-interest/work - 20\n",
            "https://slate.com/human-interest/downtime - 20\n",
            "https://slate.com/human-interest/dear-prudence - 25\n",
            "https://slate.com/human-interest/care-and-feeding - 26\n",
            "https://slate.com/human-interest/how-to-do-it - 25\n",
            "https://www.wnd.com/ - 63\n",
            "https://www.wnd.com/category/front-page/politics/ - 20\n",
            "https://www.wnd.com/category/front-page/us/ - 20\n",
            "https://www.wnd.com/category/front-page/world/ - 20\n",
            "https://www.wnd.com/category/front-page/faith/ - 20\n",
            "https://www.wnd.com/category/front-page/health/ - 20\n",
            "https://www.wnd.com/category/diversions/ - 20\n",
            "https://www.wnd.com/category/front-page/education/ - 20\n",
            "https://www.wnd.com/category/opinion/commentary/ - 20\n",
            "https://www.wnd.com/category/money/ - 20\n",
            "2787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting Thumbnails**"
      ],
      "metadata": {
        "id": "uk9uAn0yEmie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.exceptions import HTTPError\n",
        "\n",
        "def fetch_thumbnail_url(news_url):\n",
        "    try:\n",
        "        response = requests.get(news_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        thumbnail_tag = soup.find('meta', property='og:image')\n",
        "\n",
        "        if thumbnail_tag and 'content' in thumbnail_tag.attrs:\n",
        "            thumbnail_url = thumbnail_tag['content']\n",
        "            return thumbnail_url\n",
        "    except requests.HTTPError as e:\n",
        "        print(f\"Error fetching thumbnail for URL: {news_url}. Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while fetching thumbnail for URL: {news_url}. Error: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# Read the JSON file\n",
        "with open('/content/Green_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Process each entry and fetch the thumbnail URL\n",
        "for entry in data:\n",
        "    news_url = entry['url']\n",
        "    thumbnail_url = fetch_thumbnail_url(news_url)\n",
        "    entry['thumbnail'] = thumbnail_url\n",
        "\n",
        "# Write the updated data to a new JSON file\n",
        "with open('output_1.json', 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "print(\"New JSON file created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuPp3wumEjb7",
        "outputId": "15b69091-8822-4109-e36a-1b78c4c99a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New JSON file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activist** "
      ],
      "metadata": {
        "id": "2lOGfMlp-RPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "To Get the news urls into a json file \n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "from newspaper import Config, Article\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "news_urls = [\n",
        "    \"https://www.activistpost.com/category/activism/page/2\",\n",
        "    \"https://www.activistpost.com/category/activism/page/3\",\n",
        "    \"https://www.activistpost.com/category/activism/page/4\",\n",
        "    \"https://www.activistpost.com/category/health/page/2\",\n",
        "    \"https://www.activistpost.com/category/health/page/3\",\n",
        "    \"https://www.activistpost.com/category/health/page/4\",\n",
        "    \"https://www.activistpost.com/category/environment/page/2\",\n",
        "    \"https://www.activistpost.com/category/environment/page/3\",\n",
        "    \"https://www.activistpost.com/category/environment/page/4\",\n",
        "    \"https://www.activistpost.com/category/technology/page/2\",\n",
        "    \"https://www.activistpost.com/category/technology/page/3\",\n",
        "    \"https://www.activistpost.com/category/technology/page/4\",\n",
        "    \"https://www.activistpost.com/category/war/page/2\",\n",
        "    \"https://www.activistpost.com/category/war/page/3\",\n",
        "    \"https://www.activistpost.com/category/war/page/4\",\n",
        "    \"https://www.activistpost.com/category/environment/page/5\",\n",
        "    \"https://www.activistpost.com/category/environment/page/6\",\n",
        "    \"https://www.activistpost.com/category/environment/page/7\",\n",
        "    \"https://www.activistpost.com/category/environment/page/8\",\n",
        "    \"https://www.activistpost.com/category/environment/page/9\",\n",
        "    \"https://www.activistpost.com/category/technology/page/4\",\n",
        "    \"https://www.activistpost.com/category/technology/page/5\",\n",
        "    \"https://www.activistpost.com/category/technology/page/6\",\n",
        "    \"https://www.activistpost.com/category/technology/page/7\",\n",
        "    \"https://www.activistpost.com/category/technology/page/8\",\n",
        "    \"https://www.activistpost.com/category/technology/page/9\",\n",
        "    \"https://www.activistpost.com/category/technology/page/10\",\n",
        "    \"https://www.activistpost.com/category/war/page/5\",\n",
        "    \"https://www.activistpost.com/category/war/page/6\",\n",
        "    \"https://www.activistpost.com/category/war/page/7\",\n",
        "    \"https://www.activistpost.com/category/war/page/8\",\n",
        "    \"https://www.activistpost.com/category/war/page/9\"]\n",
        "\n",
        "def extract_headlines(url):\n",
        "    config = Config()\n",
        "    config.request_timeout = 10\n",
        "    config.browser_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "\n",
        "    article = Article(url, config=config)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "\n",
        "    soup = BeautifulSoup(article.html, 'html.parser')\n",
        "\n",
        "    headlines = []\n",
        "    for headline in soup.find_all('a'):\n",
        "        if headline.text.strip() != '':\n",
        "            headline_url = headline['href'] if 'href' in headline.attrs else ''\n",
        "            absolute_url = urljoin(url, headline_url)\n",
        "\n",
        "            # Check if the headline has at least 10 words\n",
        "            words = headline.text.strip().split()\n",
        "            if len(words) >= 10:\n",
        "                headlines.append({\n",
        "                    \"title\": headline.text.strip(),\n",
        "                    \"url\": absolute_url\n",
        "                })\n",
        "\n",
        "    return headlines\n",
        "\n",
        "def get_random_headlines(news_urls, count):\n",
        "    random_headlines = []\n",
        "    article_counts = {}  # Store the counts of articles scraped from each website\n",
        "\n",
        "    for url in news_urls:\n",
        "        headlines = extract_headlines(url)\n",
        "        random_headlines.extend(random.sample(headlines, min(count, len(headlines))))\n",
        "\n",
        "        # Update the article count for the current website\n",
        "        article_counts[url] = len(headlines)\n",
        "\n",
        "        # Check if the desired count has been reached\n",
        "        if len(random_headlines) >= count:\n",
        "            break\n",
        "\n",
        "    return random_headlines[:count], article_counts\n",
        "\n",
        "# Get 1000 random headlines and article counts\n",
        "random_headlines, article_counts = get_random_headlines(news_urls, 100000)\n",
        "\n",
        "# Save headlines to JSON file\n",
        "output_file = \"Activist_data.json\"\n",
        "with open(output_file, 'w') as file:\n",
        "    json.dump(random_headlines, file, indent=4)\n",
        "\n",
        "print(\"Random headlines saved to\", output_file)\n",
        "\n",
        "# Print article counts\n",
        "counts = 0\n",
        "print(\"Article Counts:\")\n",
        "for url, count in article_counts.items():\n",
        "    print(url, \"-\", count)\n",
        "    counts+=count\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS5ahQaw-VTM",
        "outputId": "cc26eea9-b46e-4662-f91f-46dc6a394fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random headlines saved to Activist_data.json\n",
            "Article Counts:\n",
            "https://www.activistpost.com/category/activism/page/2 - 21\n",
            "https://www.activistpost.com/category/activism/page/3 - 21\n",
            "https://www.activistpost.com/category/activism/page/4 - 21\n",
            "https://www.activistpost.com/category/health/page/2 - 18\n",
            "https://www.activistpost.com/category/health/page/3 - 22\n",
            "https://www.activistpost.com/category/health/page/4 - 22\n",
            "https://www.activistpost.com/category/environment/page/2 - 23\n",
            "https://www.activistpost.com/category/environment/page/3 - 24\n",
            "https://www.activistpost.com/category/environment/page/4 - 25\n",
            "https://www.activistpost.com/category/technology/page/2 - 21\n",
            "https://www.activistpost.com/category/technology/page/3 - 22\n",
            "https://www.activistpost.com/category/technology/page/4 - 21\n",
            "https://www.activistpost.com/category/war/page/2 - 18\n",
            "https://www.activistpost.com/category/war/page/3 - 23\n",
            "https://www.activistpost.com/category/war/page/4 - 21\n",
            "https://www.activistpost.com/category/environment/page/5 - 25\n",
            "https://www.activistpost.com/category/environment/page/6 - 22\n",
            "https://www.activistpost.com/category/environment/page/7 - 23\n",
            "https://www.activistpost.com/category/environment/page/8 - 23\n",
            "https://www.activistpost.com/category/environment/page/9 - 21\n",
            "https://www.activistpost.com/category/technology/page/5 - 20\n",
            "https://www.activistpost.com/category/technology/page/6 - 23\n",
            "https://www.activistpost.com/category/technology/page/7 - 23\n",
            "https://www.activistpost.com/category/technology/page/8 - 24\n",
            "https://www.activistpost.com/category/technology/page/9 - 22\n",
            "https://www.activistpost.com/category/technology/page/10 - 22\n",
            "https://www.activistpost.com/category/war/page/5 - 20\n",
            "https://www.activistpost.com/category/war/page/6 - 21\n",
            "https://www.activistpost.com/category/war/page/7 - 22\n",
            "https://www.activistpost.com/category/war/page/8 - 24\n",
            "https://www.activistpost.com/category/war/page/9 - 21\n",
            "679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from newspaper import Config, Article\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def append_thumbnail(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        news_data = json.load(file)\n",
        "\n",
        "    config = Config()\n",
        "    config.request_timeout = 10\n",
        "    config.browser_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "\n",
        "    for entry in news_data:\n",
        "        url = entry['url']\n",
        "\n",
        "        article = Article(url, config=config)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        soup = BeautifulSoup(article.html, 'html.parser')\n",
        "\n",
        "        entry['thumbnail'] = None\n",
        "\n",
        "        entry_content_div = soup.find('div', class_='entry-content clearfix')\n",
        "        if entry_content_div:\n",
        "            first_paragraph = entry_content_div.find('p')\n",
        "            if first_paragraph and first_paragraph.find('img'):\n",
        "                thumbnail_url = first_paragraph.find('img')['src']\n",
        "                entry['thumbnail'] = thumbnail_url\n",
        "\n",
        "    with open(json_file, 'w') as file:\n",
        "        json.dump(news_data, file, indent=4)\n",
        "\n",
        "    print(\"Thumbnail URLs appended to the JSON file.\")\n",
        "\n",
        "# Specify the path to your JSON file\n",
        "json_file_path = \"/content/Activist_data.json\"\n",
        "\n",
        "# Append thumbnail URLs to the JSON file\n",
        "append_thumbnail(json_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbrX4tkLE8Nr",
        "outputId": "6df90206-e95e-4148-ba7e-8b026033b39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thumbnail URLs appended to the JSON file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Judicial Data**"
      ],
      "metadata": {
        "id": "Yc738CWZ_-Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "To Get the news urls into a json file \n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "from newspaper import Config, Article\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "news_urls = [\n",
        "    \"https://www.judicialwatch.org\",\n",
        "    \"https://www.judicialwatch.org/?taxonomy=category\",\n",
        "    ]\n",
        "\n",
        "def extract_headlines(url):\n",
        "    config = Config()\n",
        "    config.request_timeout = 10\n",
        "    config.browser_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "\n",
        "    article = Article(url, config=config)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "\n",
        "    soup = BeautifulSoup(article.html, 'html.parser')\n",
        "\n",
        "    headlines = []\n",
        "    for headline in soup.find_all('a'):\n",
        "        if headline.text.strip() != '':\n",
        "            headline_url = headline['href'] if 'href' in headline.attrs else ''\n",
        "            absolute_url = urljoin(url, headline_url)\n",
        "\n",
        "            # Check if the headline has at least 10 words\n",
        "            words = headline.text.strip().split()\n",
        "            if len(words) >= 10:\n",
        "                headlines.append({\n",
        "                    \"title\": headline.text.strip(),\n",
        "                    \"url\": absolute_url\n",
        "                })\n",
        "\n",
        "    return headlines\n",
        "\n",
        "def get_random_headlines(news_urls, count):\n",
        "    random_headlines = []\n",
        "    article_counts = {}  # Store the counts of articles scraped from each website\n",
        "\n",
        "    for url in news_urls:\n",
        "        headlines = extract_headlines(url)\n",
        "        random_headlines.extend(random.sample(headlines, min(count, len(headlines))))\n",
        "\n",
        "        # Update the article count for the current website\n",
        "        article_counts[url] = len(headlines)\n",
        "\n",
        "        # Check if the desired count has been reached\n",
        "        if len(random_headlines) >= count:\n",
        "            break\n",
        "\n",
        "    return random_headlines[:count], article_counts\n",
        "\n",
        "# Get 1000 random headlines and article counts\n",
        "random_headlines, article_counts = get_random_headlines(news_urls, 100000)\n",
        "\n",
        "# Save headlines to JSON file\n",
        "output_file = \"JudicialWatch_data.json\"\n",
        "with open(output_file, 'w') as file:\n",
        "    json.dump(random_headlines, file, indent=4)\n",
        "\n",
        "print(\"Random headlines saved to\", output_file)\n",
        "\n",
        "# Print article counts\n",
        "counts = 0\n",
        "print(\"Article Counts:\")\n",
        "for url, count in article_counts.items():\n",
        "    print(url, \"-\", count)\n",
        "    counts+=count\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7TrbhKN_AhB",
        "outputId": "cd8b4a1a-0aac-4487-84fb-437124a29304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random headlines saved to JudicialWatch_data.json\n",
            "Article Counts:\n",
            "https://www.judicialwatch.org - 44\n",
            "https://www.judicialwatch.org/?taxonomy=category - 27\n",
            "71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from newspaper import Config, Article\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_thumbnail_url(url):\n",
        "    config = Config()\n",
        "    config.request_timeout = 10\n",
        "    config.browser_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "\n",
        "    article = Article(url, config=config)\n",
        "    try:\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        soup = BeautifulSoup(article.html, 'html.parser')\n",
        "\n",
        "        container_featured_image = soup.find('div', class_='container-featured-image')\n",
        "        if container_featured_image:\n",
        "            img_tag = container_featured_image.find('img', src=True)\n",
        "            if img_tag:\n",
        "                thumbnail_url = img_tag['src']\n",
        "                return thumbnail_url\n",
        "            else:\n",
        "                print(\"No thumbnail found for URL:\", url)\n",
        "        else:\n",
        "            print(\"No thumbnail container found for URL:\", url)\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching thumbnail for URL:\", url)\n",
        "        print(\"Error:\", str(e))\n",
        "\n",
        "    return None\n",
        "\n",
        "def append_thumbnail(json_file):\n",
        "    with open(json_file, 'r+') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        for entry in data:\n",
        "            url = entry['url']\n",
        "            thumbnail_url = fetch_thumbnail_url(url)\n",
        "            entry['thumbnail'] = thumbnail_url\n",
        "\n",
        "    output_file = \"output_3.json\"\n",
        "    with open(output_file, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "    print(\"Thumbnail URLs appended to\", output_file)\n",
        "\n",
        "# Provide the path to your input JSON file\n",
        "json_file_path = \"/content/JudicialWatch_data.json\"\n",
        "\n",
        "# Append thumbnail URLs to the JSON file and save as output_2.json\n",
        "append_thumbnail(json_file_path)\n"
      ],
      "metadata": {
        "id": "SdJD6Rbk_7Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining All JSON Data Files"
      ],
      "metadata": {
        "id": "RD0m2qHaXwUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def append_json_files(input_files, output_file):\n",
        "    data = []\n",
        "    \n",
        "    # Read contents from each input file\n",
        "    for file_name in input_files:\n",
        "        with open(file_name, 'r') as file:\n",
        "            json_data = json.load(file)\n",
        "            data.extend(json_data)\n",
        "    \n",
        "    # Append the data to the output file\n",
        "    with open(output_file, 'a') as file:\n",
        "        json.dump(data, file)\n",
        "        file.write('\\n')\n",
        "\n",
        "# Example usage\n",
        "input_files = ['/content/output_1.json', '/content/JudicialWatch_data.json', '/content/output_3.json']\n",
        "output_file = 'FinalData.json'\n",
        "append_json_files(input_files, output_file)\n"
      ],
      "metadata": {
        "id": "gAwnNEqGRDh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhdqDAd5YUA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}