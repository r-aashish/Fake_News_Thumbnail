{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBI+grvWOPc0CI4DXCFx3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-aashish/Fake_News_Thumbnail/blob/main/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade googletrans==4.0.0rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piNCzPDIYvlT",
        "outputId": "bcbc446f-8dbc-4d0c-8102-90dab6dba7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==4.0.0rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2022.12.7)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.*\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.*\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.3.0)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17413 sha256=ef76265dc023008b08e334faf71005092a380691a3bfda756c7766416f32e1fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "from httpx import Timeout\n",
        "\n",
        "# Load the dataset\n",
        "input_file = \"/content/labeled_data.json\"\n",
        "output_file = \"output_multi_languages.json\"\n",
        "column_to_translate = \"title\"\n",
        "languages = [\"bn\", \"hi\", \"te\",\"es\",\"de\",\"zh-TW\"]\n",
        "\n",
        "with open(input_file, \"r\") as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(json_data)\n",
        "\n",
        "# Create a Translator object\n",
        "translator = Translator(service_urls=['translate.google.com'], timeout=Timeout(50))\n",
        "\n",
        "# Loop through each row in the dataset and translate the specified column to multiple languages\n",
        "for index, row in df.iterrows():\n",
        "    for lang in languages:\n",
        "        try:\n",
        "            column_name = f\"{column_to_translate}_{lang}\"\n",
        "            original_text = row[column_to_translate]\n",
        "            translated_text = translator.translate(original_text, src=\"en\", dest=lang).text\n",
        "            df.at[index, column_name] = translated_text\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# Save the translated dataset to a new JSON file\n",
        "translated_json_data = df.to_dict(orient=\"records\")\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(translated_json_data, f, ensure_ascii=False, indent=2) "
      ],
      "metadata": {
        "id": "oSsJki3aMPYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the names of the input JSON files\n",
        "input_files = [\"/content/labeled_data.json\",\"/content/output_bn.json\", \"/content/output_de.json\" , \"/content/output_es.json\", \"/content/output_hi.json\", \"/content/output_te.json\", \"/content/output_zh-TW.json\"]\n",
        "\n",
        "combined_data = []\n",
        "\n",
        "# Loop through each file and read its content\n",
        "for input_file in input_files:\n",
        "    with open(input_file, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "        combined_data.extend(data)\n",
        "\n",
        "# Save the combined data to a single output JSON file\n",
        "with open(\"combined_output.json\", \"w\") as output_file:\n",
        "    json.dump(combined_data, output_file, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "_6-RkGk2DuRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fq2g40Qhdqpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Load the input JSON file\n",
        "with open(\"/content/combined_output.json\", \"r\") as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(data)\n",
        "\n",
        "# Calculate the indices for splitting\n",
        "length = len(data)\n",
        "index_80 = int(length * 0.8)\n",
        "index_90 = int(length * 0.9)\n",
        "\n",
        "# Split the data into train, test, and validation parts\n",
        "train_data = data[:index_80]\n",
        "test_data = data[index_80:index_90]\n",
        "val_data = data[index_90:]\n",
        "\n",
        "# Save the split data to separate JSON files\n",
        "with open(\"Multi_train_data.json\", \"w\") as output_file:\n",
        "    json.dump(train_data, output_file, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(\"Multi_test_data.json\", \"w\") as output_file:\n",
        "    json.dump(test_data, output_file, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(\"Multi_val_data.json\", \"w\") as output_file:\n",
        "    json.dump(val_data, output_file, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "fv7kB2FCG6WX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}